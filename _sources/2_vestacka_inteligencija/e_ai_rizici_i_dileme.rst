Ризици и дилеме у вези са вештачком интелигенцијом
==================================================

Поред многобројних очигледних користи које доноси развој вештачке интелигенције, у јавности повремено 
могу да се чују одређена питања и примети забринутост. Централно питање које се поставља је да ли, 
односно на које све начине и под којим условима вештачка интелигенција може да угрози одређене 
појединце или друштвене групе, па и цео људски род. 

**Интелигентно оружје**

Један од страхова који прати примену вештачке интелигенције тиче се и самосталног смртоносног оружја 
(енгл. lethal autonomous weapon) - оружја које може да лоцира, препозна и усмрти жртву без људске 
интервенције. Стварање оваквих робота би могло да промени схватање оружаних конфликата, страдања и 
одговорности и да трајно нашкоди хуманој страни друштва. Постоје `организације 
<https://en.wikipedia.org/wiki/Campaign_to_Stop_Killer_Robots>`_ и појединци који се залажу за 
забрану стварања овакве врсте оружја, као и за за контролу робота у примени силе кроз детаљнију 
правну регулативу. Са друге стране, познато је да више десетина земаља већ годинама истражује употребу 
робота у борбеним дејствима.

**Злоупотреба надзора и праћења**

Технологије засноване на ВИ могу да се користе и за надгледање у безбедносне сврхе. Препознавање 
лица и гласа омогућава масовно праћење, па владе појединих земаља могу да улажу средства у ову област 
са образложењем да ће се смањити ризик од шпијуна, терориста и других непријатеља државе. Могуће је, 
међутим, да се такав систем надзора и праћења употреби и против широких маса и претвори у "дигиталну 
диктатуру". Уз напредак у домену разумевања текстуалних садржаја, на пример, онога што се чита, 
претражује, размењује путем поште или оставља на форумима у коментарима, могућ је увид у интересовања 
и потребе појединаца, њихова осећања и ставове, што улази далеко у приватност појединаца. 

**Морална и правна одговорност**

Многа отворена питања прате и употребу аутономних возила. Аутономна возила, као и људи, у критичним 
ситуацијама треба да доносе одлуке које могу да доведу до несреће. Такве одлуке је, такође, потребно 
научити. Не постоји ни један довољно етички оквир који би покрио овај аспект учења, јер истраживања 
указују да постоје велике културолошке разлике у ставовима људи по овим питањима. Једно такво 
истраживање је онлајн платформа `морална машина <https://en.wikipedia.org/wiki/Moral_Machine>`_, 
која испитанике поставља пред `моралне дилеме <https://en.wikipedia.org/wiki/Ethical_dilemma>`_, у 
којима између два нежељена исхода треба изабрати један. Творци система аутономних возила не могу да 
избегну уградњу својих ставова о овим осетљивим питањима у систем (став да не треба непосредно и 
свесно утицати на одлуке ове врсте је такође став).

Осим етичке, поставља се и питање правне одговорности. Није јасно да ли у случају несреће одговорност 
треба да се припише власнику аутономног возила, лицу за управљачем, произвођачу или неком четвртом. 
Многе земље убрзано разрађују законе којима се регулишу правна питања у вези са ВИ, али јасно је да 
ће при овако брзом развоју догађаја правни оквири мање или више каснити за реалним потребама.

**Алгоритамска пристрасност**

Програми вештачке интелигенције могу да постану пристрасни након учења на основу података из реалног 
света. Пристрасност може да се унесе начином на који се бирају подаци за учење, па дизајнери система и 
програмери не морају ни да буду свесни да пристрасност постоји. Ако систем учи одређене особине људи, 
могуће је нпр. да подаци за учење не чине репрезентативан скуп, тј. да одређене друштвене групе нису 
заступљене уопште, или су заступљене несразмерно стварној популацији, тако да систем о тој групи може 
да доноси пристрасне процене, јер су засноване на сувише малом узорку. Реални примери се односе на 
примене ВИ у процени кандидата за посао, процени подобности са добијање кредита, или процени понашања 
оптужених или осуђених. Потенцијално пристрасан систем може да дискриминише одређене друштвене групе 
или појединце, ускраћујући им право на једнаку прилику, односно једнак третман.

**Незапосленост**

Вероватна последица развоја ВИ је да ће се смањити број радних места одређеног типа, пошто ће многе 
послове моћи да обављају интелигентне машине. Као што су механизација и аутоматизација смањиле потребу 
за запосленима на великим земљишним поседима, фабрикама, градилиштима, рудницима и слично, вештачка 
интелигенција би могла у скоријој будућности да угрози послове као што су кувари брзе хране, возачи, 
рачуновође, адвокатски помоћници, а у нешто даљој будућности и друге "канцеларијске" послове (адвокати, 
банкари, консултанати, лекари, инжењери, менаџери, научници). Постоје велика неслагања у проценама, 
тако да није јасно колики је стваран ризик од незапослености у овим доменима и када би до тога могло 
да дође. Последице смањења потражње за одређеним пословима би могле да буду и позитивне у случају да 
компаније које профитирају применом ВИ одлуче да уложе средства у подизање нивоа услуга, што би 
покренуло нове послове.

